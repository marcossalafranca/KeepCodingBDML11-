{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj8bcxFX85UO"
      },
      "source": [
        "# Práctica Final: detección de mensajes troll\n",
        "\n",
        "## Ejemplo de solución.\n",
        "\n",
        "**Disclaimer.** Existen distintas formas de resolver el ejercicio y todas se considerarán correctas siempre que el modelo se entrene y despliegue acorde a lo esperado.\n",
        "\n",
        "En los recientes años, Twitch no sólo ha surgido como un pionero en la comunicación moderna, sino también como una un sitio donde las nuevas generaciones pasan el tiempo ver a otros jovenes haciendo cosas por internet. Esta plataforma, rica en interactividad, permite a los usuarios compartir sus pensamientos en tiempo real y después de las transmisiones. Sin embargo, entre las voces genuinas, algunas buscan dañar con comentarios ofensivos.\n",
        "\n",
        "Pero hoy, nos embarcamos en una misión inspiradora: diseñar una Inteligencia Artificial que pueda identificar y neutralizar estos mensajes tóxicos. A lo largo de esta práctica, no solo entrenaremos un modelo de Deep Learning avanzado, sino que también lo implementaremos para inferencias en batch, estableciendo un nuevo estándar en la industria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgE9XB8ACVmQ"
      },
      "source": [
        "# Configuración de nuestro proyecto en GCP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAorVw7RCT9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9167b0-d75c-4b71-f91d-6f435c538785"
      },
      "source": [
        "PROJECT_ID = \"\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbAl8pSkCXCm"
      },
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el bucket mediante la consola y una vez creado lo indicamos en la variable:"
      ],
      "metadata": {
        "id": "p5Xoej7vH0X-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Zpx14FCfXy"
      },
      "source": [
        "BUCKET_NAME = \"\" #@param {type:\"string\"}\n",
        "REGION = \"europe-west1\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE4V-Xt0ChSb"
      },
      "source": [
        "! gsutil ls -al gs://$BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8Qw14y_nRa"
      },
      "source": [
        "# Entrenamiento e inferencia en Batch\n",
        "\n",
        "## Preparación\n",
        "\n",
        "Para esta primera parte se va a utilizar [Tweets Dataset for Detection of Cyber-Trolls](https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls). El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0). **Las métricas obtenidas del entrenamiento y la inferencia no se tendrán en cuenta para la evaluación de la práctica, la importancia está en la arquitectura de la solución**, es decir, lo importante no es que nuestro modelo detecte correctamente los tweets de trolls si no que funcione y sea capaz de hacer inferencias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM_dh-471gIq"
      },
      "source": [
        "A continuación, se van a subir los datos de entrenamiento al bucket del proyecto que se haya creado. **Importante:** crea el bucket en una única región. Os dejo disponibilizado el dataset en un bucket de acceso público:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqp4L_nmUjAc"
      },
      "source": [
        "%pip install gdown\n",
        "! gdown \"1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCQQ9j2I11tg"
      },
      "source": [
        "Ahora se crea el directorio dónde vas a desarrollar esta primera parte de la práctica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsblBlJ6RrGm"
      },
      "source": [
        "%mkdir /content/batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyK51quU1_oi"
      },
      "source": [
        "Se establece el directorio de trabajo que hemos creado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ybSaotRwkP"
      },
      "source": [
        "import os\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd /content/batch\n",
        "\n",
        "WORK_DIR = os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSd4bAo2aj_"
      },
      "source": [
        "Ahora se descargarán los datos en el workspace de Colab para trabajar en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOmUJSg1JCgg"
      },
      "source": [
        "! gdown \"1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\"\n",
        "! mv dataset-cybertrolls.json dataset.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRoW6zyg2kEz"
      },
      "source": [
        "Se establecen las dependencias que se usarán en la práctica. Se pueden añadir y quitar las dependencias que no se usen o viceversa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s0-8_JK_z2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fdc06e-1202-499b-8a21-b8f6c94f6a75"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "apache-beam[gcp]==2.24.0\n",
        "tensorflow\n",
        "gensim==3.6.0\n",
        "fsspec==0.8.4\n",
        "gcsfs==0.7.1\n",
        "numpy==1.20.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIl8lrPp2x2j"
      },
      "source": [
        "Instalamos las dependencias. **No olvides reiniciar el entorno al instalar y establecer las variables y credenciales de GCP al arrancar.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "74dHb1iQ0UNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install apache-beam[gcp]"
      ],
      "metadata": {
        "id": "jRCMqESUnUcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DDu27S3CJv"
      },
      "source": [
        "## Primer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de preprocesamiento utilizando Apache Beam para generar datos de train, eval y test para los datos proporcionados anteriormente. Requisitos:\n",
        "\n",
        "- Proporcionar dos modos de ejecución: `train` y `test`\n",
        "- Soportar ejecuciones en local con `DirectRunner` y ejecuciones en Dataflow usando `DataFlowRunner`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1blctlxs_dPO"
      },
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "# Aquí va tu código\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawuKC0k4B0e"
      },
      "source": [
        "Se proporciona un fichero `setup.py` necesario para ejecutar en DataFlow. Modificar la variable `REQUIRED_PACKAGES` con las dependencias que se hayan usado en el `requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1MQvWsk_mVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef946bf4-8db6-4c79-cc42-3e1cf4a41684"
      },
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "  \"apache-beam[gcp]==2.24.0\",\n",
        "  \"tensorflow==2.8.0\",\n",
        "  \"gensim==3.6.0\",\n",
        "  \"fsspec==0.8.4\",\n",
        "  \"gcsfs==0.7.1\",\n",
        "  \"numpy==1.20.0\"\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"twitchstreaming\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    description=\"Troll detection\",\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Me creo una copia por si hubiera algún error en el procesamiento (buena práctica):"
      ],
      "metadata": {
        "id": "hxYFYMy-o55h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown \"1dTaKofC9ZcMWa5cVtGLDFkEnbc4hiPJr\"\n",
        "! gsutil cp dataset-cybertrolls.json //$WORK_DIR/data.json\n"
      ],
      "metadata": {
        "id": "KlBCbvSQpBUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDlz_fL4UJA"
      },
      "source": [
        "### Validación preprocess train en local\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFpirg37C3bN"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkkG271a4qnA"
      },
      "source": [
        "### Validación preprocess test en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4lkLgecLS3i"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZM9Sbj45LK"
      },
      "source": [
        "## Segundo ejercicio\n",
        "\n",
        "Desarrollar una tarea de entrenamiento para los datos preprocesados. Requisitos:\n",
        "\n",
        "- Soportar ejecuciones en local usando el SDK de AI-Platform y ejecuciones en GCP con el mismo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUwXgm_5el-"
      },
      "source": [
        "Se crea el directorio donde trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMi8dI1gLoIc"
      },
      "source": [
        "%mkdir /content/batch/trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dJyMXTuNPwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "521448c5-da2a-4301-8c5d-155b2484af62"
      },
      "source": [
        "%%writefile trainer/__init__.py\n",
        "\n",
        "version = \"0.1.0\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing trainer/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUu6gKaTL_S2"
      },
      "source": [
        "%%writefile trainer/task.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "# Aquí va tu código"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8axPHdHX6d9W"
      },
      "source": [
        "### Validación Train en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos preprocesados del apartado anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9997ZzsLmq-"
      },
      "source": [
        "# Explicitly tell `gcloud ai-platform local train` to use Python 3\n",
        "! gcloud config set ml_engine/local_python $(which python3)\n",
        "\n",
        "# This is similar to `python -m trainer.task --job-dir local-training-output`\n",
        "# but it better replicates the AI Platform environment, especially for\n",
        "# distributed training (not applicable here).\n",
        "! gcloud ai-platform local train \\\n",
        "  --package-path trainer \\\n",
        "  --module-name trainer.task \\\n",
        "  -- \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --epochs 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nhlkcjn62Zo"
      },
      "source": [
        "## Tercer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de inferencia utilizando Apache Beam para generar predicciones usando los modelos generados en el apartado anterior así como los datos de test generados en el primer ejercicio.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHBpLXB-OmjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100c6c55-24d1-408c-ce9f-d604aba6d332"
      },
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "\n",
        "# Aquí va tu código"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdgcLIP7a42"
      },
      "source": [
        "Generamos un timestamp para la ejecución de las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pvKMtuQPOlr"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLMdMup70ne"
      },
      "source": [
        "### Validación Predict en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta inferencia usando los modelos anteriores y los datos de test generados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUJN0XCLPR_X"
      },
      "source": [
        "! python3 predict.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --model-dir $WORK_DIR/model \\\n",
        "  batch \\\n",
        "  --inputs-dir $WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um68jx_F8mjF"
      },
      "source": [
        "## Extra. Comprobaciones en la nube\n",
        "\n",
        "En esta última parte se validará el funcionamiento del código en un proyecto de GCP sobre DataFlow y AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Me copio los datos necesarios a Google Cloud:"
      ],
      "metadata": {
        "id": "crPjZuOVuM-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "xN16t4Myukcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil cp -r model gs://$BUCKET_NAME"
      ],
      "metadata": {
        "id": "Ybu7kzf7uR1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil cp -r transformed_data gs://$BUCKET_NAME"
      ],
      "metadata": {
        "id": "6jSwelYiu5p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JHC9dCT83FH"
      },
      "source": [
        "Establecemos el bucket y region de GCP sobre el que trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgUpX__KQvt-"
      },
      "source": [
        "GCP_WORK_DIR = 'gs://' + BUCKET_NAME\n",
        "GCP_REGION = \"europe-west1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A66WAfm9DmU"
      },
      "source": [
        "### Validación preprocess train en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en GCP con el servicio DataFlow.\n",
        "\n",
        "**Nota.** Recuerda detener la celda una vez el trabajo te aparezca ejecutando en DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYVuWyJP1Ya"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8czKRMb99ZRX"
      },
      "source": [
        "### Validación preprocess test en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en GCP con el servicio DataFlow.\n",
        "\n",
        "**Nota.** Recuerda detener la celda una vez el trabajo te aparezca ejecutando en DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45mfSRGdQLBO"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Omitimos el entrenamiento en la nube para no incurrir en costes innecesarios."
      ],
      "metadata": {
        "id": "sTVj6bqII_3m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5SOWsy9886"
      },
      "source": [
        "### Validación predict en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la predicción correcta de los datos de test usando los modelos generados en el comando anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXoHCgMg-LUD"
      },
      "source": [
        "Generamos un timestamp para el almacenamiento de las inferencias en Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmcetQtnQjVo"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota.** Recuerda detener la celda una vez el trabajo te aparezca ejecutando en DataFlow."
      ],
      "metadata": {
        "id": "qRsZaneP0waK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT_NxsKDQlxD"
      },
      "source": [
        "# For using sample models: --model-dir gs://$BUCKET_NAME/models/\n",
        "! python3 predict.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --model-dir $GCP_WORK_DIR/model/ \\\n",
        "  batch \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --inputs-dir $GCP_WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $GCP_WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mensaje final\n",
        "\n",
        "¡Muchas gracias por participar en este curso, espero que tanto las sesiones teóricas como la práctica te hayan resultado útiles. A lo largo de esta semmana iréis recibiendo feedback personalizado sobre vuestras prácticas.\n",
        "\n",
        "¡Muchas gracias y ánimo con el proyecto final!"
      ],
      "metadata": {
        "id": "co3YBKjscCVI"
      }
    }
  ]
}